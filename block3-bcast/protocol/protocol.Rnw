\documentclass[a4paper,10pt]{article}

\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[usenames,dvipsnames]{color}
\usepackage{comment}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage[pdfborder={0 0 0}]{hyperref}
\usepackage{spverbatim}
\usepackage{booktabs}
\usepackage{graphicx}

\definecolor{OliveGreen}{cmyk}{0.64,0,0.95,0.40}
\definecolor{Gray}{gray}{0.5}

\lstset{
    language=C,
    basicstyle=\ttfamily,
    keywordstyle=\color{OliveGreen},
    commentstyle=\color{Gray},
    captionpos=b,
    breaklines=true,
    breakatwhitespace=false,
    showspaces=false,
    showtabs=false,
    numbers=left,
}

\title{VU High Performance Computing \\
       SS 2013 \\
       Block 3: MPI Library Implementation \\
       Broadcast Algorithms}
\author{Jakob Gruber, 0203440}

\begin{document}

\maketitle
\tableofcontents
\pagebreak

\section{Introduction} \label{section:introduction}

In this assignment, we implemented three different broadcast algorithms and
subsequently benchmarked and compared these against the \lstinline|MPI_Bcast|
implementation in NEC MPI.

The \emph{linear pipeline} algorithm partitions the data range into small segments,
and structures all processes in a linear queue. Root begins by transmitting
the first segment, and in each subsequent round each process simultaneously receives
one segment from its predecessor and sends the segment received in the previous
round on to its successor.

The \emph{pipelined binary tree} is similar, except processes are structured as a
binary tree. A round consists of each process receiving a segment from its parent,
and passing the previous segment to both child processes.

In the \emph{binomial tree} algorithm, the properties of a binomial tree
are exploited in order to maximize the work done by all processes during each round.
Unlike the others, this algorithm is not pipelined.

\section{Results}

The following results were obtained by benchmarking with NECmpi on the Jupiter system. The
application was compiled using gcc 4.4.7 with

\begin{verbatim}
CFLAGS = -Wall -Wextra -pedantic -std=c99 -D_GNU_SOURCE -O3
\end{verbatim}

A benchmark run can be started by using the \verb|make bench| and \verb|make bench-bs|
targets in our makefile. Results are then written to \verb|results/bench.csv|.

Random communicators were created using the following code:

\begin{lstlisting}
srand(rank);
MPI_Comm_split(MPI_COMM_WORLD, 1, rand(), &comm);
\end{lstlisting}

\begin{comment}
TODO: "Verify/falsify claims of lecture"
Show graphs.
\end{comment}

<<results=tex, echo=FALSE>>=
\SweaveInput{functions.Rnw}
plotResults("../results/bench.csv")
@

\section{Discussion}

For small and medium data sizes, the binary tree algorithm
wins fairly obviously over all processor counts. It is then superceded by both the pipelined
linear and binomial algorithms for larger data sets.

Our results show that a binary tree for data sizes of up to $8 \cdot 10^6$ bytes combined
with a linear pipeline for larger volumes seems to be the best combination, outperforming
the native \lstinline|MPI_Bcast| implementation at high thread counts and large data volumes.

\subsection{Block Size}

Assuming $\alpha = 1 \mu s, \beta = 0.002 \mu s / byte$ (taken from the benchmark graphs in the lecture slides),
the optimal number of segments for the linear pipeline algorithm is

\begin{displaymath}
M = \sqrt{0.002m(p + 1)}
\end{displaymath}

For $m = 8000000 bytes, p = 576, M = 3038$, which evaluates to a block size of $2633$ bytes
(or 329 integers). For $m = 111111111 \cdot 8, M = 32028$ with a block size of 3470 bytes
or 433 integers.

In order to determine an optimal block size, we benchmarked both pipelined
algorithms with block sizes ranging from 512 integers (= 4096 bytes on a 64 bit system)
to 65536 integers (= 524288 bytes).

The experimental results were somewhat surprising:
small (512, 1024 integers) and large (65536 integers) block sizes performed well,
while sizes such as 4096 integers caused heavy slowdowns.

We chose a block size of 1024 integers (= 4096 bytes) for both algorithms
since they performed the best in practice.

% TODO: Graphs :(

\subsection{Random Communicators}

<<results=tex, echo=FALSE>>=
plotResults("../results/rbench.csv")
@

\vspace{3mm}

The extra network cost incurred by the use of random communicators is mostly noticeable in the
performance of linear and binomial algorithms with low to medium data volumes. While all
runtimes suffer, these two algorithms suddenly increase by an order of magnitude for
data sizes of up to $8 \cdot 10^6$ bytes. This makes sense, since both exploit network proximity
between nodes far more than the binary tree algorithm does.

The main problem of random communicators is that the communication cost between processors
is not equal. Some might be cheap (if they're located on the same node), while others are
expensive. Effective algorithms need to take this into account and try to minimize the usage of
slow network links.

However, MPI does not provide any means to negate the performance problems of random communicators;
there are no ways to determine properties of the network topology unless one relies
on other, external methods. Within MPI itself, one has to rely on the communicator being
well-formed, meaning that consecutive ranks lie next to each other in the network topology.

\end{document}
